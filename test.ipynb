{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from simulator.game.connect import Config  # type: ignore[import]\n",
    "\n",
    "from alphazero_implementation.models.games.connect4 import CNNModel\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# from simulator.textual.examples.agent import AgentApp, RandomAgent\n",
    "\n",
    "config = Config(6, 7, 4)\n",
    "\n",
    "path = \"/Users/pveron/Code/alphazero-implementation/checkpoints/run_150/model-epoch=2199.ckpt\"\n",
    "path = \"/Users/pveron/Code/alphazero-implementation/checkpoints/run_156/model-epoch=1999.ckpt\"\n",
    "path = \"/Users/pveron/Code/alphazero-implementation/lightning_logs/alphazero/run_168_iter200_episodes100_sims100/checkpoints/epoch=1999-step=1431360.ckpt\"\n",
    "\n",
    "model = CNNModel.load_from_checkpoint(  # type: ignore[arg-type]\n",
    "    path,\n",
    "    height=config.height,\n",
    "    width=config.width,\n",
    "    max_actions=config.width,\n",
    "    num_players=config.num_players,\n",
    ")\n",
    "# model = CNNModel(  # type: ignore[arg-type]\n",
    "#     height=config.height,\n",
    "#     width=config.width,\n",
    "#     max_actions=config.width,\n",
    "#     num_players=config.num_players,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.38212865591049194, 0.018891004845499992, 0.03477592393755913, 0.008441480807960033, 0.004113901872187853, 0.3447142243385315, 0.20693479478359222])\n",
      "[0.07656644284725189, -0.0765664279460907]\n"
     ]
    }
   ],
   "source": [
    "state = config.sample_initial_state()\n",
    "[policy], [value] = model.predict([state])\n",
    "\n",
    "print(policy.values())\n",
    "print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'count': 4, 'height': 6, 'width': 7},\n",
       " 'grid': [[-1, -1, -1, -1, -1, -1, -1],\n",
       "  [-1, -1, -1, -1, -1, -1, -1],\n",
       "  [-1, -1, -1, -1, -1, -1, -1],\n",
       "  [-1, -1, -1, -1, -1, -1, -1],\n",
       "  [-1, -1, -1, -1, -1, -1, -1],\n",
       "  [-1, -1, -1, -1, -1, -1, -1]],\n",
       " 'player': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {\n",
    "    \"config\": {\"count\": 4, \"height\": 6, \"width\": 7},\n",
    "    \"grid\": [\n",
    "        [0, 0, -1, 1, -1, 0, -1],\n",
    "        [-1, -1, -1, 1, -1, -1, -1],\n",
    "        [-1, -1, -1, 1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1],\n",
    "    ],\n",
    "    \"player\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulator.game.connect import State\n",
    "\n",
    "state = State.from_json(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.010334096848964691, 0.0022563186939805746, 0.0013825338101014495, 0.006581863854080439, 0.00029155719676055014, 0.9764062762260437, 0.0027472558431327343])\n",
      "[-0.5397583246231079, 0.5397583842277527]\n"
     ]
    }
   ],
   "source": [
    "[policy], [value] = model.predict([state])\n",
    "\n",
    "print(policy.values())\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphazero_implementation.mcts.agent import MCTSAgent\n",
    "\n",
    "agent = MCTSAgent(model, 2, 100, config.sample_initial_state())\n",
    "\n",
    "model.eval()\n",
    "\n",
    "[episode_1, episode_2] = agent.generate_episodes()\n",
    "\n",
    "# print(episode)\n",
    "# print(episode.samples[4])\n",
    "#     pass\n",
    "#     print(episode)\n",
    "#     print(episode.samples[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1,  1,  1,  1,  0,  0, -1],\n",
       "        [-1, -1,  0, -1, -1,  0, -1],\n",
       "        [-1, -1, -1, -1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1, -1, -1]], dtype=int8),\n",
       " True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_1.samples[-1].state.grid, episode_1.samples[-1].state.has_ended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1,  1,  1,  1,  0,  0, -1],\n",
       "        [-1, -1, -1,  0, -1,  0, -1],\n",
       "        [-1, -1, -1, -1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1, -1, -1]], dtype=int8),\n",
       " True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_2.samples[-1].state.grid, episode_2.samples[-1].state.has_ended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in episode_2.samples[-1].state.actions:\n",
    "    print(action.column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_2.samples[-1].state.has_ended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphazero_implementation.mcts.mcgs import Node\n",
    "\n",
    "path = \"/Users/pveron/Code/alphazero-implementation/checkpoints/run_156/model-epoch=1999.ckpt\"\n",
    "path = \"/Users/pveron/Code/alphazero-implementation/lightning_logs/alphazero/run_168_iter200_episodes100_sims100/checkpoints/epoch=1999-step=1431360.ckpt\"\n",
    "\n",
    "\n",
    "model = CNNModel.load_from_checkpoint(  # type: ignore[arg-type]\n",
    "    path,\n",
    "    height=config.height,\n",
    "    width=config.width,\n",
    "    max_actions=config.width,\n",
    "    num_players=config.num_players,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "json = {\n",
    "    \"config\": {\"count\": 4, \"height\": 6, \"width\": 7},\n",
    "    \"grid\": [\n",
    "        [0, 0, 0, -1, -1, 1, -1],\n",
    "        [-1, -1, -1, 1, -1, 1, -1],\n",
    "        [-1, -1, -1, 1, -1, 1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1],\n",
    "    ],\n",
    "    \"player\": 0,\n",
    "}\n",
    "\n",
    "state = State.from_json(json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = model.predict([state])[0][0]\n",
    "action = max(policy.items(), key=lambda x: x[1])[0]\n",
    "action.column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, -1, -1,  1, -1],\n",
       "       [ 0, -1, -1,  1, -1,  1, -1],\n",
       "       [-1, -1, -1,  1, -1,  1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1]], dtype=int8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.sample_next_state().grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = MCTSAgent(\n",
    "    model,\n",
    "    num_episodes=100,\n",
    "    simulations_per_episode=100,\n",
    "    initial_state=state,\n",
    ")\n",
    "policy = agent.compute_policy(Node(state), {})\n",
    "action = max(policy.items(), key=lambda x: x[1])[0]\n",
    "action.column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, -1, -1,  1, -1],\n",
       "       [ 0, -1, -1,  1, -1,  1, -1],\n",
       "       [-1, -1, -1,  1, -1,  1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1]], dtype=int8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.sample_next_state().grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, -1, -1,  1, -1],\n",
       "       [-1, -1, -1,  1, -1,  1, -1],\n",
       "       [-1, -1, -1,  1, -1,  1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1]], dtype=int8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json = {\n",
    "    \"config\": {\"count\": 4, \"height\": 6, \"width\": 7},\n",
    "    \"grid\": [\n",
    "        [0, 0, 0, -1, -1, 1, -1],\n",
    "        [-1, -1, -1, 1, -1, 1, -1],\n",
    "        [-1, -1, -1, 1, -1, 1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1],\n",
    "    ],\n",
    "    \"player\": 0,\n",
    "}\n",
    "\n",
    "state = State.from_json(json)\n",
    "state.grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1.], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.action_at(3).sample_next_state().reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<simulator.game.connect.Action at 0x2cdbac270>: 0.9494949494949495,\n",
       " <simulator.game.connect.Action at 0x2cdbad190>: 0.020202020202020204,\n",
       " <simulator.game.connect.Action at 0x2cdbaf8f0>: 0.020202020202020204,\n",
       " <simulator.game.connect.Action at 0x2cdbafa70>: 0.010101010101010102}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from simulator.game.connect import Action\n",
    "\n",
    "from alphazero_implementation.mcts.mcgs import Node\n",
    "\n",
    "json = {\n",
    "    \"config\": {\"count\": 4, \"height\": 6, \"width\": 7},\n",
    "    \"grid\": [\n",
    "        [0, 0, 0, -1, -1, 1, -1],\n",
    "        [-1, -1, -1, 1, -1, 1, -1],\n",
    "        [-1, -1, -1, 1, -1, 1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1],\n",
    "    ],\n",
    "    \"player\": 0,\n",
    "}\n",
    "\n",
    "state = State.from_json(json)\n",
    "\n",
    "current_node = Node(state)\n",
    "\n",
    "for _ in range(100):\n",
    "    node: Node = current_node\n",
    "    path: list[tuple[Node, Action]] = []\n",
    "    nodes_by_state = {}\n",
    "\n",
    "    while True:\n",
    "        if node.is_terminal:\n",
    "            node.utility_values = node.game_state.reward.tolist()  # type: ignore[attr-defined]\n",
    "            # Add terminal nodes to leaf_nodes for backpropagation\n",
    "            break\n",
    "        elif node.visit_count == 0:  # New node not yet visited\n",
    "            [policy], [value] = model.predict([node.game_state])\n",
    "            node.action_policy = policy\n",
    "            node.utility_values = value\n",
    "            break\n",
    "        else:\n",
    "            action = node.select_action()\n",
    "            if action not in node.children_and_edge_visits:\n",
    "                new_game_state = action.sample_next_state()\n",
    "\n",
    "                if new_game_state in nodes_by_state:\n",
    "                    child = nodes_by_state[new_game_state]\n",
    "                else:\n",
    "                    child = Node(game_state=new_game_state)\n",
    "                    nodes_by_state[new_game_state] = child\n",
    "\n",
    "                node.children_and_edge_visits[action] = (child, 0)\n",
    "            else:\n",
    "                child = node.children_and_edge_visits[action][0]\n",
    "            path.append((node, action))\n",
    "            node = child\n",
    "\n",
    "    node.visit_count = 1\n",
    "    node.cumulative_value = node.utility_values[node.game_state.player]\n",
    "\n",
    "    # Backpropagate all paths\n",
    "    for parent, action in reversed(path):\n",
    "        # Update edge visits\n",
    "        child, edge_visits = parent.children_and_edge_visits[action]\n",
    "        parent.children_and_edge_visits[action] = (child, edge_visits + 1)\n",
    "\n",
    "        # Update visit count and cumulative value\n",
    "        children_and_edge_visits = parent.children_and_edge_visits.values()\n",
    "        parent.visit_count = 1 + sum(\n",
    "            edge_visits for (_, edge_visits) in children_and_edge_visits\n",
    "        )\n",
    "        parent.cumulative_value = (1 / parent.visit_count) * (\n",
    "            parent.utility_values[parent.game_state.player]\n",
    "            + sum(\n",
    "                child.cumulative_value * edge_visits\n",
    "                for (child, edge_visits) in children_and_edge_visits\n",
    "            )\n",
    "        )\n",
    "\n",
    "policy = {\n",
    "    action: edge_visits / (current_node.visit_count - 1)\n",
    "    for action, (\n",
    "        _,\n",
    "        edge_visits,\n",
    "    ) in current_node.children_and_edge_visits.items()\n",
    "}\n",
    "policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, -1, -1,  1, -1],\n",
       "       [-1, -1, -1,  1, -1,  1, -1],\n",
       "       [-1, -1, -1,  1, -1,  1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1]], dtype=int8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_node.game_state.grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, [-0.7607280015945435, 0.7607279419898987], array([0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[policy], [value] = model.predict(\n",
    "    [current_node.game_state.action_at(3).sample_next_state()]\n",
    ")\n",
    "policy, value, current_node.game_state.reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "  )\n",
       "  (shared_layers): Sequential(\n",
       "    (0): Linear(in_features=10752, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (policy_head): Linear(in_features=512, out_features=7, bias=True)\n",
       "  (value_head): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from simulator.game.connect import Action, Config, State\n",
    "\n",
    "from alphazero_implementation.models.games.connect4 import CNNModel\n",
    "\n",
    "config = Config(6, 7, 4)\n",
    "\n",
    "\n",
    "path = \"/Users/pveron/Code/alphazero-implementation/checkpoints/run_156/model-epoch=1999.ckpt\"\n",
    "path = \"/Users/pveron/Code/alphazero-implementation/lightning_logs/alphazero/run_168_iter200_episodes100_sims100/checkpoints/epoch=1999-step=1431360.ckpt\"\n",
    "\n",
    "model = CNNModel.load_from_checkpoint(  # type: ignore[arg-type]\n",
    "    path,\n",
    "    height=config.height,\n",
    "    width=config.width,\n",
    "    max_actions=config.width,\n",
    "    num_players=config.num_players,\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = config.sample_initial_state()\n",
    "json = {\n",
    "    \"config\": {\"count\": 4, \"height\": 6, \"width\": 7},\n",
    "    \"grid\": [\n",
    "        [0, 0, 0, -1, -1, 1, -1],\n",
    "        [-1, -1, -1, -1, -1, 1, -1],\n",
    "        [-1, -1, -1, -1, -1, 1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1],\n",
    "    ],\n",
    "    \"player\": 0,\n",
    "}\n",
    "\n",
    "state = State.from_json(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_policy(state: State) -> dict[Action, float]:\n",
    "    [policy], _ = model.predict([state])\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphazero_implementation.mcts.agent import MCTSAgent\n",
    "\n",
    "\n",
    "def improved_policy_v1(state: State) -> dict[Action, float]:\n",
    "    mcts = MCTSAgent(model, 1, 100, state)\n",
    "\n",
    "    episodes = mcts.run()\n",
    "    policy = episodes[0].samples[-1].policy\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphazero_implementation.mcts_v2.mcts import AlphaZeroMCTS\n",
    "from alphazero_implementation.mcts_v2.node import Node\n",
    "\n",
    "\n",
    "def improved_policy_v2(state: State) -> dict[Action, float]:\n",
    "    mcts = AlphaZeroMCTS(model)\n",
    "\n",
    "    root = Node(state)\n",
    "    policy = mcts.run(root, 100)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:\n",
      "[[ 0  0  0 -1 -1  1 -1]\n",
      " [-1 -1 -1 -1 -1  1 -1]\n",
      " [-1 -1 -1 -1 -1  1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1]]\n",
      "\n",
      "Raw policy from model:\n",
      "{<simulator.game.connect.Action object at 0x2cdbad450>: 0.002391638932749629, <simulator.game.connect.Action object at 0x2cdbafc30>: 0.00013282954751048237, <simulator.game.connect.Action object at 0x2cdbae430>: 0.0016656157094985247, <simulator.game.connect.Action object at 0x2cdbad570>: 0.6138741374015808, <simulator.game.connect.Action object at 0x2cdbaca10>: 0.0007002202328294516, <simulator.game.connect.Action object at 0x2cdbae1d0>: 0.3806518316268921, <simulator.game.connect.Action object at 0x2cdbaddb0>: 0.0005836421623826027}\n",
      "\n",
      "run took 0.23 seconds to run.\n",
      "Improved policy from MCTS v1:\n",
      "{<simulator.game.connect.Action object at 0x2cdb8dc90>: 0.0056179775280898875, <simulator.game.connect.Action object at 0x2cdb8ccd0>: 0.9943820224719101}\n",
      "\n",
      "Improved policy from MCTS v2:\n",
      "{<simulator.game.connect.Action object at 0x174ecfbb0>: 0.0, <simulator.game.connect.Action object at 0x2cdbaf930>: 0.0, <simulator.game.connect.Action object at 0x2cdbaea30>: 0.0, <simulator.game.connect.Action object at 0x2cdbad4f0>: 0.96, <simulator.game.connect.Action object at 0x2cdbae950>: 0.0, <simulator.game.connect.Action object at 0x2cdbae550>: 0.03, <simulator.game.connect.Action object at 0x2cdbafc10>: 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"State:\")\n",
    "print(state.grid)\n",
    "print()\n",
    "# Compare the three different policy functions\n",
    "state_policy = raw_policy(state)\n",
    "print(\"Raw policy from model:\")\n",
    "print(state_policy)\n",
    "print()\n",
    "\n",
    "mcts_policy_v1 = improved_policy_v1(state)\n",
    "print(\"Improved policy from MCTS v1:\")\n",
    "print(mcts_policy_v1)\n",
    "print()\n",
    "\n",
    "mcts_policy_v2 = improved_policy_v2(state)\n",
    "print(\"Improved policy from MCTS v2:\")\n",
    "print(mcts_policy_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw policy:\n",
      "0: 0.0024\n",
      "1: 0.0001\n",
      "2: 0.0017\n",
      "3: 0.6139\n",
      "4: 0.0007\n",
      "5: 0.3807\n",
      "6: 0.0006\n",
      "\n",
      "MCTS v1 policy:\n",
      "0: 0.0056\n",
      "5: 0.9944\n",
      "\n",
      "MCTS v2 policy:\n",
      "0: 0.0000\n",
      "1: 0.0000\n",
      "2: 0.0000\n",
      "3: 0.9600\n",
      "4: 0.0000\n",
      "5: 0.0300\n",
      "6: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def print_policy(policy: dict[Action, float]):\n",
    "    for action, prob in policy.items():\n",
    "        print(f\"{action.column}: {prob:.4f}\")\n",
    "\n",
    "\n",
    "print(\"Raw policy:\")\n",
    "print_policy(state_policy)\n",
    "print()\n",
    "\n",
    "print(\"MCTS v1 policy:\")\n",
    "print_policy(mcts_policy_v1)\n",
    "print()\n",
    "\n",
    "print(\"MCTS v2 policy:\")\n",
    "print_policy(mcts_policy_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
