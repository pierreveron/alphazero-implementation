from __future__ import annotations

import numpy as np
import torch
import torch.nn.functional as F
from torch import nn

from alphazero_simple.connect4_game import Connect4Game

from .base_model import BaseModel


class ResBlock(nn.Module):
    def __init__(self, num_channels: int):
        super().__init__()
        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(num_channels)
        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(num_channels)

    def forward(self, x):
        residual = x
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.bn2(self.conv2(x))
        x += residual
        x = F.relu(x)
        return x


class ResNet(BaseModel):
    def __init__(
        self,
        board_size: tuple[int, int],
        action_size: int,
        num_res_blocks: int,
        num_channels: int,
    ):
        super(ResNet, self).__init__()  # type: ignore[no-untyped-call]

        self.rows, self.cols = board_size
        self.action_size = action_size

        # Initial convolution
        self.input_conv = nn.Sequential(
            nn.Conv2d(3, num_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(num_channels),
            nn.ReLU(),
        )

        # Residual blocks
        self.residual_blocks = nn.ModuleList(
            [ResBlock(num_channels) for _ in range(num_res_blocks)]
        )

        # Policy head
        self.policy_head = nn.Sequential(
            nn.Conv2d(num_channels, 32, kernel_size=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(32 * self.rows * self.cols, self.action_size),
        )

        # Value head from https://github.com/foersterrobert/AlphaZeroFromScratch
        self.value_head = nn.Sequential(
            nn.Conv2d(num_channels, 3, kernel_size=3, padding=1),
            nn.BatchNorm2d(3),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(3 * self.rows * self.cols, 1),
            # nn.Tanh(),
        )

        # Value head generated by Cursor
        # self.value_head = nn.Sequential(
        #     nn.Conv2d(num_channels, 32, kernel_size=1),
        #     nn.BatchNorm2d(32),
        #     nn.ReLU(),
        #     nn.Flatten(),
        #     nn.Linear(32 * self.rows * self.cols, 256),
        #     nn.ReLU(),
        #     nn.Dropout(0.3),
        #     nn.Linear(256, 1),
        # )

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        x = self.input_conv(x)

        for block in self.residual_blocks:
            x = block(x)

        action_logits = self.policy_head(x)
        value_logit = self.value_head(x)

        # return action_logits, value_logit
        return action_logits, value_logit.view(-1)

    # def _states_to_tensor(self, boards: list[np.ndarray]) -> Tensor:
    #     batch_size = len(boards)
    #     inputs = torch.zeros((batch_size, 3, self.rows, self.cols))

    #     for i, board in enumerate(boards):
    #         # Create the three channels
    #         empty_squares = (board == 0).astype(np.float32)
    #         current_player = (board == 1).astype(np.float32)
    #         opponent = (board == -1).astype(np.float32)

    #         # Stack the channels for this board
    #         inputs[i] = torch.FloatTensor(
    #             np.stack([empty_squares, current_player, opponent])
    #         )

    #     return inputs.to(self.device)

    def _states_to_tensor(self, boards: list[np.ndarray]) -> torch.Tensor:
        # Stack boards into a single array
        boards_array = np.stack(boards)

        # Create all channels at once using broadcasting
        empty_squares = (boards_array == 0).astype(np.float32)
        current_player = (boards_array == 1).astype(np.float32)
        opponent = (boards_array == -1).astype(np.float32)

        # Stack all channels at once
        inputs = np.stack([empty_squares, current_player, opponent], axis=1)

        # Convert to tensor and move to device in one operation
        return torch.from_numpy(inputs)

    def predict(self, boards: list[np.ndarray]) -> tuple[list[np.ndarray], list[float]]:
        """
        Input: list of boards as numpy arrays of shape (rows, cols)
        Returns: probability distributions over actions and value estimates
        """
        game = Connect4Game()
        # Convert list of boards to tensor
        boards_tensor = self._states_to_tensor(boards).to(self.device)

        self.eval()
        with torch.no_grad():
            action_logits, value_logit = self.forward(boards_tensor)

            # Get valid moves for each board and mask invalid moves
            valid_moves = torch.tensor(
                [game.get_valid_moves(board) for board in boards], device=self.device
            )
            action_logits = action_logits.masked_fill(valid_moves == 0, float("-inf"))

            # Apply softmax only on valid moves
            action_probs = F.softmax(action_logits, dim=1)
            value = torch.tanh(value_logit)

        # Convert to list of numpy arrays
        action_probs_list = list(action_probs.cpu().numpy())
        value_list = list(value.cpu().numpy().flatten())

        return action_probs_list, value_list
